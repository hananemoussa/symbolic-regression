_wandb:
    value:
        cli_version: 0.22.3
        e:
            kdtqb3hs102cg0wyc0ceu6tgzozin2yv:
                args:
                    - task=bin
                    - model=llama32
                    - train=dpo
                    - cluster=osc_ascend
                    - seed=0
                    - prefix=test_run
                    - gpu_nums=0
                    - num_cont_rounds=50
                    - finetuning_frequency=10
                    - one_tuning=0
                    - max_loops=5
                    - wandb=1
                    - project=evotune-reproducing
                    - entity=hananenmoussa
                    - run_or_dev=run
                codePath: src/experiments/main.py
                codePathLocal: src/experiments/main.py
                cpu_count: 96
                cpu_count_logical: 96
                cudaVersion: "12.4"
                disk:
                    /:
                        total: "204010946560"
                        used: "32987938816"
                executable: /users/PAA0201/hananemoussa/.conda/envs/evotune/bin/python
                git:
                    commit: 9c55ee21a4459e8601bf9b7d61ef2cc9871d6012
                    remote: https://github.com/hananemoussa/EvoTune.git
                gpu: NVIDIA A100-SXM4-80GB
                gpu_count: 1
                gpu_nvidia:
                    - architecture: Ampere
                      cudaCores: 6912
                      memoryTotal: "85899345920"
                      name: NVIDIA A100-SXM4-80GB
                      uuid: GPU-025dd8c5-db98-0607-090d-e9ceb0de82ae
                host: a0014.ten.osc.edu
                memory:
                    total: "1081776664576"
                os: Linux-5.14.0-427.76.1.el9_4.x86_64-x86_64-with-glibc2.34
                program: /fs/ess/PAA0201/hananemoussa/EvoTune/src/experiments/main.py
                python: CPython 3.10.19
                root: /fs/ess/PAA0201/hananemoussa/EvoTune
                slurm:
                    cluster_name: ascend
                    conf: /var/spool/slurmd/conf-cache/slurm.conf
                    cpus_on_node: "1"
                    cpus_per_task: "1"
                    distribution: pack
                    gpus: "1"
                    gpus_on_node: "1"
                    gtids: "0"
                    job_account: paa0201
                    job_cpus_per_node: "1"
                    job_end_time: "1762834653"
                    job_gid: "6044"
                    job_gpus: "2"
                    job_id: "2837454"
                    job_name: interactive
                    job_nodelist: a0014
                    job_num_nodes: "1"
                    job_partition: quad
                    job_qos: ascend-default
                    job_start_time: "1762820253"
                    job_uid: "46117"
                    job_user: hananemoussa
                    jobid: "2837454"
                    launch_node_ipaddr: 192.148.247.181
                    localid: "0"
                    mpi_type: pmi2
                    nnodes: "1"
                    nodeid: "0"
                    nodelist: a0014
                    nprocs: "1"
                    ntasks: "1"
                    pmi2_proc_mapping: (vector,(0,1,1))
                    pmi2_srun_port: "39203"
                    pmi2_step_nodes: a0014
                    prio_process: "0"
                    procid: "0"
                    pty_port: "34111"
                    pty_win_col: "152"
                    pty_win_row: "37"
                    script_context: prolog_task
                    srun_comm_host: 192.148.247.181
                    srun_comm_port: "38715"
                    step_id: "4294967290"
                    step_launcher_port: "38715"
                    step_nodelist: a0014
                    step_num_nodes: "1"
                    step_num_tasks: "1"
                    step_tasks_per_node: "1"
                    stepid: "4294967290"
                    submit_dir: /fs/ess/PAA0201/hananemoussa
                    submit_host: ascend-login02.hpc.osc.edu
                    task_pid: "2492849"
                    tasks_per_node: "1"
                    time_limit: "14400"
                    topology_addr: a0014
                    topology_addr_pattern: node
                    tres_per_task: cpu=1
                startedAt: "2025-11-11T02:46:02.732230Z"
                writerId: kdtqb3hs102cg0wyc0ceu6tgzozin2yv
        m: []
        python_version: 3.10.19
        t:
            "1":
                - 1
                - 5
                - 11
                - 12
                - 41
                - 49
                - 50
                - 51
                - 53
                - 71
                - 84
                - 95
                - 98
            "2":
                - 1
                - 5
                - 11
                - 12
                - 41
                - 49
                - 50
                - 51
                - 53
                - 71
                - 84
                - 95
                - 98
            "3":
                - 2
                - 13
                - 16
            "4": 3.10.19
            "5": 0.22.3
            "6": 4.57.1
            "10":
                - 20
            "12": 0.22.3
            "13": linux-x86_64
accelerate_config:
    value: 1gpu_0
cluster:
    value:
        scratch_path: $PFSDIR
        use_tgi: 0
        use_vllm: 1
creative_prompt:
    value: 1
descending_order:
    value: 1
entity:
    value: hananenmoussa
eval_frequency:
    value: 100
evalset:
    value: trainperturbedset
final_percentile:
    value: 0.2
finetuning_frequency:
    value: 10
flash_attn:
    value: 0
full_accelerate_config:
    value: ./configs/accelerate_config/1gpu_0.yaml
full_model_name:
    value: meta-llama/Llama-3.2-1B-Instruct
function_str_to_extract:
    value: priority
gpu_nums:
    value: 0
group_name:
    value: test_run/bin_llama32_dpo
initial_percentile:
    value: 0.6
logs_dir:
    value: out/logs/test_run/bin_bin_llama32_dpo_0
logs_path:
    value: .
lora_config:
    value:
        lora_alpha: 32
        r: 64
lr_annealing:
    value: 0
max_loops:
    value: 5
model:
    value:
        max_tokens: 2048
        model_name: llama32
        temperature: 0.9
        topk: 100
        topp: 0.95
model_adapter_dir:
    value: out/logs/test_run/bin_bin_llama32_dpo_0/model_adapter_llama32
model_dtype:
    value: bfloat16
multiple_models:
    value: 0
num_cont_rounds:
    value: 50
num_outputs_per_prompt:
    value: 8
num_rounds:
    value: 2701
num_workers:
    value: 12
one_tuning:
    value: 0
percentile:
    value: 70
prefix:
    value: test_run
programdatabaseConfig:
    value:
        functions_per_prompt: 2
        num_islands: 6
        temp: 40
        temp_sampling_flag: 1
project:
    value: evotune-reproducing
run_identifier_name:
    value: bin_llama32_dpo
run_or_dev:
    value: run
seed:
    value: 0
task:
    value:
        OR: 1
        Weibull: 0
        failed_score: -20000
        function_str_to_extract: priority
        init_best_fit: 1
        initial_percentile: 0.3
        mem_limit_gb: 10
        programdatabaseConfig:
            temp: 40
        task_name: bin
        timeout_period: 60
testset:
    value: testset
top_k_functions_for_test:
    value: 50
train:
    value:
        dpo_config:
            beta: 0.4
            f_alpha_divergence_coef: 1
            f_divergence_type: alpha_divergence
            gradient_accumulation_steps: 16
            gradient_checkpointing: 1
            learning_rate: 1e-05
            logging_steps: 1
            lr_scheduler_type: cosine
            max_seq_length: 6500
            num_train_epochs: 2
            per_device_train_batch_size: 2
            warmup_steps: 0
            weight_decay: 0.001
        dpo_strategy: 2
        finetuning_frequency: 400
        train_method_name: dpo
use_tgi:
    value: 0
use_vllm:
    value: 1
wandb:
    value: 1
wandb_name:
    value: test_run/bin_llama32_dpo_0
